### SGD and Mini-Batches

> Now that we have a loss function that is suitable for driving SGD, we can consider some of the details involved in the next phase of the learning process, which is to change or update the weights based on the gradients. This is called an _optimization step_.

So optimization step is the *ACTION* you take after you calculate the gradients. Got it.

There's this concept of Mini Batch - a compromise between training over the entire dataset (too long), and training on one example (imprecise and unstable gradient). Batch Sizing is all about what you'll learn, at the cost of speed of training.

> A larger batch size means that you will get a more accurate and stable estimate of your dataset's gradients from the loss function, but it will take longer, and you will process fewer mini-batches per epoch. Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model quickly and accurately. We will talk about how to make this choice throughout this book.

Looks like there are also performance reasons to do this:

> Another good reason for using mini-batches rather than calculating the gradient on individual data items is that, in practice, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time, so it's helpful if we can give them lots of data items to work on. Using mini-batches is one of the best ways to do this. However, if you give them too much data to work on at once, they run out of memory—making GPUs happy is also tricky!

In the Chapter About Production, it seems like you might get better generalization if you mix the training data. Varying things as much as possible helps - hence shuffling.

Using Pytorches DataLoader here

```python
coll = range(15)
dl = DataLoader(coll, batch_size=5, shuffle=True)
list(dl)
# [tensor([ 3, 12,  8, 10,  2]),
# tensor([ 9,  4,  7, 14,  5]),
# tensor([ 1, 13,  0,  6, 11])]
```

Obviously, we want to wrap the inputs and outputs of a model together (dependent and independent variables). We can do this in Dataset in Pytorch using Tuples.

```python
ds = L(enumerate(string.ascii_lowercase))
ds
# (#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]
```

> When we pass a `Dataset` to a `DataLoader` we will get back mini-batches which are themselves tuples of tensors representing batches of independent and dependent variables:

```python
dl = DataLoader(ds, batch_size=6, shuffle=True)
list(dl)
# [(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),
# (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')), 
# etc...
```

We are now ready to write our first training loop for a model using SGD!!!

## Putting It All Together

> ... In code, our process will be implemented something like this for each epoch:

```python
for x,y in dl:
	pred = model(x)
	loss = loss_func(pred, y)
	loss.backward()
	parameters -= parameters.grad * lr
```

Step 1: Initialize Weights!

```python
weights = init_params((28*28,1))
bias = init_params(1)
```

Create DataLoader using a Dataset for training and validation!

```python
# Train
dl = DataLoader(dset, batch_size=256)
xb,yb = first(dl) # fastai for grabbing first item
xb.shape,yb.shape
# (torch.Size([256, 784]), torch.Size([256, 1]))
```

```python
# Valid
valid_dl = DataLoader(valid_dset, batch_size=256)
```

Creating a mini batch for testing

```python
batch = train_x[:4]
batch.shape
```

To be Continued...