## Putting It All Together

> ... In code, our process will be implemented something like this for each epoch:

```python
for x,y in dl:
	pred = model(x)
	loss = loss_func(pred, y)
	loss.backward()
	parameters -= parameters.grad * lr
```

Step 1: Initialize Weights!

```python
weights = init_params((28*28,1))
bias = init_params(1)
```

Create DataLoader using a Dataset for training and validation!

```python
# Train
dl = DataLoader(dset, batch_size=256)
xb,yb = first(dl) # fastai for grabbing first item
xb.shape,yb.shape
# (torch.Size([256, 784]), torch.Size([256, 1]))
```

```python
# Valid
valid_dl = DataLoader(valid_dset, batch_size=256)
```

Creating a mini batch for testing (of 4)

```python
batch = train_x[:4]
batch.shape
```

get the predictions for the batch

```python
preds = linear1(batch)
preds
```

calculate the loss for the batch

```python
loss = mnist_loss(preds, train_y[:4])
loss
```

from the loss, calculate the gradient

```python
loss.backward()
weights.grad.shape, weights.grad.mean(),bias.grad
# (torch.Size([784, 1]), tensor(-0.0010), tensor([-0.0069]))
```

Putting this entire thing into a function

```python
def calc_grad(xb, yb, model):
	preds = model(xb) # get predictions
	loss = mnist_loss(preds, yb) # calculate loss
	loss.backward() # calculate gradient
```

^ this is great. However, running `loss.backward()` adds the gradient everytime, so if we call the function multiple times, we will continually add the gradient.

```python
calc_grad(batch, train_y[:4], linear1)
weights.grad.mean(),bias.grad
# (tensor(-0.0020), tensor([-0.0137]))


calc_grad(batch, train_y[:4], linear1)
weights.grad.mean(),bias.grad
# (tensor(-0.0029), tensor([-0.0206]))
```

Let's set the gradients to 0, because we don't want to continually add the gradient.

```python
weights.grad.zero_();
bias.grad.zero_();
```

^ this operation is in place, any operation with an underscore in [[PyTorch]] is in place.

Finally, the last remaining step is to update the weights and biases based on the gradient x learning rate.

> When we do so, we have to tell PyTorch not to take the gradient of this step too—otherwise things will get very confusing when we try to compute the derivative at the next batch! If we assign to the `data` attribute of a tensor then PyTorch will not take the gradient of that step. Here's our basic training loop for an epoch:

```python
def train_epoch(model, lr, params):
	for xb,yb in dl:
		calc_grad(xb, yb, model)
		for p in params:
			p.data -= p.grad*lr # update weights based on gradient x learning rate
			p.grad.zero_() # reset the gradient!
```

> We also want to check how we're doing, by looking at the accuracy of the validation set. To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0. So our accuracy for each item can be calculated (using broadcasting, so no loops!) with:

```python
(preds>0.0).float() == train_y[:4]
```

Cool! Negative = 7, Positive = 3

The function to calculate to calculate validation accuracy

```python
def batch_accuracy(xb, yb):
	preds = xb.sigmoid() # squish outputs into something simple
	correct = preds(>0.5) == yb
	return correct.float().mean()
```

Now to calculate the accuracy for all of the batches of validation

```python
def validate_epoch(model):
	accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]
	return round(torch.stack(accs).mean().item(), 4)
```


Let's train one epoch!

```python
lr = 1. # learning rate
params = weights,bias # get the params
train_epoch(linear1, lr, params) #
validate_epoch(linear1)
```

That was good! Now let's do a few more.

```python
for i in range(20):
	train_epoch(linear1, lr, params)
	print(validate_epoch(linear1), end=' ') # end added to print diff between nums
```

Let's build this in Pytorch, which is called an ***Optimizer***

### Creating an Optimizer

> Because this is such a general foundation, PyTorch provides some useful classes to make it easier to implement. The first thing we can do is replace our `linear1` function with PyTorch's `nn.Linear` module. A _module_ is an object of a class that inherits from the PyTorch `nn.Module` class. Objects of this class behave identically to standard Python functions, in that you can call them using parentheses and they will return the activations of a model.

> `nn.Linear` does the same thing as our `init_params` and `linear` together. It contains both the _weights_ and _biases_ in a single class. Here's how we replicate our model from the previous section:

```python
linear_model = nn.Linear(28*28,1) # linear classifier
```

Every PyTorch module knows what params it has that can be trained, which are available through the parameters method

```python
w,b = linear_model.parameters()
w.shape,b.shape
# (torch.Size([1, 784]), torch.Size([1]))
```

We can use this Information to build an [[Optimizer]]

```python
class BasicOptim:

	def __init__(self,params,lr): self.params,self.lr = list(params),lr

	def step(self, *args, **kwargs):
		for p in self.params: p.data -= p.grad.data * self.lr # similar to earlier, weights and bias

	def zero_grad(self, *args, **kwargs):
		for p in self.params: p.grad = None # tell the model to not update the gradients
```

The reason we call zero_grad is because we would be using stale gradients if not! We need new gradients for each new mini batch. Just a reminder!

```python
opt = BasicOptim(linear_model.parameters(), lr) # using class to create basic optimizer
```

New Simplified Training Loop!

```python
def train_epoch(model):
	for xb,yb in dl:
		calc_grad(xb, yb, model)
		opt.step()
		opt.zero_grad()
```

And our validation loop doesn't need to change at all!

```python
validate_epoch(linear_model)
```

Now, let's put the training loop into a function to make it even simpler

```python
def train_model(model, epochs):
	for i in range(epochs):
		train_epoch(model)
		print(validate_epoch(model), end=' ')
```

Results are exactly the same as previous

```python
train_model(linear_model, 20)
```

So neat!

> fastai provides the `SGD` class which, by default, does the same thing as our `BasicOptim`:

```python
linear_model = nn.Linear(28*28,1)
opt = SGD(linear_model.parameters(), lr)
train_model(linear_model, 20)
```

> fastai also provides `Learner.fit`, which we can use instead of `train_model`. To create a `Learner` we first need to create a `DataLoaders`, by passing in our training and validation `DataLoader`s:


```python
dls = DataLoaders(dl, valid_dl) # creating the dataloaders
```

Finally, to create a learner, we need to pass in all of the elements we have created in this chapter:
1. Dataloaders
2. Model
3. Optimization Function (which will be passed the params)
4. Any metrics to print

```python
learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)
```

Then we can call learn.fit()

```python
learn.fit(10, lr=lr)
```

This function above essentially prints out a df of the train loss, validation loss, batch accuracy, and time.

## Adding a Nonlinearity

The model we have used so far is super simple, a Linear Classifier. It's very constrained, as it can only learn linear relationships (which is usually not very helpful).

To make things a little bit more complex (and to learn more tasks), we need to add something nonlinear between two linear classifiers, this is what creates a "neural network".

The entire definition of a basic neural network!

```python
def simple_net(xb):
	res = xb@w1 + b1
	res = res.max(tensor(0.0)) # Relu!
	res = res@w2 + b2
	return res
```

That's it! The simple net is two linear classifiers, with a max function between them :D

Here, `w1` and `w2` are weight tensors, and `b1` and `b2` are biases. Similar to last section, they are completely randomized.

```python
w1 = init_params((28*28,30))
b1 = init_params(30)
w2 = init_params((30,1))
b2 = init_params(1)
```

> The key point about this is that `w1` has 30 output activations (which means that `w2` must have 30 input activations, so they match). That means that the first layer can construct 30 different features, each representing some different mix of pixels. You can change that `30` to anything you like, to make the model more or less complex.

> That little function `res.max(tensor(0.0))` is called a _rectified linear unit_, also known as _ReLU_. We think we can all agree that _rectified linear unit_ sounds pretty fancy and complicated... But actually, there's nothing more to it than `res.max(tensor(0.0))`—in other words, replace every negative number with a zero. This tiny function is also available in PyTorch as `F.relu`


I Love Jeremy:

> J: There is an enormous amount of jargon in deep learning, including terms like _rectified linear unit_. The vast vast majority of this jargon is no more complicated than can be implemented in a short line of code, as we saw in this example. The reality is that for academics to get their papers published they need to make them sound as impressive and sophisticated as possible. One of the ways that they do that is to introduce jargon. Unfortunately, this has the result that the field ends up becoming far more intimidating and difficult to get into than it should be. You do have to learn the jargon, because otherwise papers and tutorials are not going to mean much to you. But that doesn't mean you have to find the jargon intimidating. Just remember, when you come across a word or phrase that you haven't seen before, it will almost certainly turn out to be referring to a very simple concept.

More Comments

> The basic idea is that by using more linear layers, we can have our model do more computation, and therefore model more complex functions. But there's no point just putting one linear layer directly after another one, because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once! That is to say, a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.

> But if we put a nonlinear function between them, such as `max`, then this is no longer true. Now each linear layer is actually somewhat decoupled from the other ones, and can do its own useful work. The `max` function is particularly interesting, because it operates as a simple `if` statement.

Mathematically, the composition of two linear functions is another linear function. THAT is crazy.

Amazingly enough, you can use math to prove that this little structure (NN) can solve any computable problem to a arbitrary high level of accuracy given you find the right params (w1, w2). And if you make these matricies enough. 

This is known as the _[[Universal Approximation Theorem]]_

> The three lines of code that we have here are known as _layers_. The first and third are known as _linear layers_, and the second line of code is known variously as a _nonlinearity_, or _activation function_

Summarizing the entire chapter, we could make this exact function using PyTorch:

```python
simple_net = nn.Sequential(
	nn.Linear(28*28,30), # layer 1
	nn.ReLU(), # layer 2
	nn.Linear(30,1) # layer 3
)
```

`nn.Sequential` calls each layer after the next.

> `nn.ReLU` is a PyTorch module that does exactly the same thing as the `F.relu` function. Most functions that can appear in a model also have identical forms that are modules. Generally, it's just a case of replacing `F` with `nn` and changing the capitalization. When using `nn.Sequential`, PyTorch requires us to use the module version. Since modules are classes, we have to instantiate them, which is why you see `nn.ReLU()` in this example.

> Because `nn.Sequential` is a module, we can get its parameters, which will return a list of all the parameters of all the modules it contains. Let's try it out! As this is a deeper model, we'll use a lower learning rate and a few more epochs.


```python
learn = Learner(dls, simple_net, opt_func=SGD,
loss_func=mnist_loss, metrics=batch_accuracy)
```

From this point on, the chapter talks about adding layers. We have finished this chapter!

## Jargon

| Term | Meaning |
| ---- | ---- |
| ReLU | Function that returns 0 for negative numbers and doesn't change positive numbers. |
| Mini-batch | A small group of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch). |
| Forward pass | Applying the model to some input and computing the predictions. |
| Loss | A value that represents how well (or badly) our model is doing. |
| Gradient | The derivative of the loss with respect to some parameter of the model. |
| Backward pass | Computing the gradients of the loss with respect to all model parameters. |
| Gradient descent | Taking a step in the directions opposite to the gradients to make the model parameters a little bit better. |
| Learning rate | The size of the step we take when applying SGD to update the parameters of the model. |
| Activations | Numbers that are calculated (both by linear and nonlinear layers) |
| Parameters | Numbers that are randomly initialized, and optimized (that is, the numbers that define the model |
| Rank Zero Tensor | Scalar |
| Rank One Tensor | Vector |
| Rank Two Tensor | Matric |
NN often have linear and non linear layers, which often alternate. yay!