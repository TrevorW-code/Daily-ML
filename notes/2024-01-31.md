[StatQuest](https://www.youtube.com/watch?v=sDv4f4s2SB8&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=4)

A little confusing.

The Chain Rule is about chaining together derivates that are interlinked. 

An example would be if you knew the relationship between time since last snack and Hunger, as well as the relationship Hunger and Craving for Ice Cream.

To find the rate of change (more specifically, of a tangent line), you might use the chain rule to simplify the complex derivative between 

Going to want to practice on Khan to make sure though.

This is commonly used for the [[Residual]] Sum of Squares as one of the [[Loss Functions]] in Machine Learning.

Residual = (Observed - Predicted)

So to solve for the weight or bias that minimizes the squared residual, you find the derivative of that squared residual with respect to the intercept.

The motivation for squared residuals seems to be that the derivative is 0 at it's lowest point. So by solving for 0, you are completely reducing the loss.