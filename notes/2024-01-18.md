
### Computing Metrics Using Broadcasting

In practice, we use accuracy rather than L1 or L2 as a "metric", because they are not very understandable to most people.

We want to calculate the metric over the validation set, so we don't accidentally overfit the model. 

Good Habit: Check Shakes as you go.

So for going to back to the handwriting function, `is_3`, we can write something that essentially finds the distance between an image (`a_3`) and the perfect 3 (`mean_3`)

We will do this using the mean absolute error

```Python
def mnist_distance(a,b): return (a-b).abs().mean((-1,-2))
mnist_distance(a_3, mean3)
```

Trev: reminder to do more pytorch math lol

If we want to do that for the entire set of validation images, we can do it like this:

```Python
valid_3_dist = mnist_distance(valid_3_tens, mean3)
valid_3_dist, valid_3_dist.shape
```

Jeremy: The magic trick is that PyTorch, when it tries to perform a simple subtraction operation between two tensors of different ranks, will use _broadcasting_. That is, it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. Broadcasting is an important capability that makes tensor code much easier to write.

Here's another example:

```Python
tensor([1,2,3]) + tensor(1) # = tensor([2, 3, 4]), crazy!
```

This is true of all broadcasting and elementwise operations and functions done in PyTorch. _It's the most important technique for you to know to create efficient PyTorch code._
- The Whole Calculation is on C, and it doesn't allocate any more memory
- up to millions of times faster on a GPU!

WOAH!

Explanation for the .mean(-1,-2):

Finally, our function calls `mean((-1,-2))`. The tuple `(-1,-2)` represents a range of axes. In Python, `-1` refers to the last element, and `-2` refers to the second-to-last. So in this case, this tells PyTorch that we want to take the mean ranging over the values indexed by the last two axes of the tensor. The last two axes are the horizontal and vertical dimensions of an image. After taking the mean over the last two axes, we are left with just the first tensor axis, which indexes over our images, which is why our final size was `(1010)`. In other words, for every image, we averaged the intensity of all the pixels in that image.

Broadcasting allows you to "predict" on all of the images at the same time! Here's code that runs the "is_3" function, determining if the image is closer to the mean of images from 3 or 7

```Python
def is_3(x): return mnist_distance(x,mean3) < mnist_distance(x,mean7)
is_3(a_3), is_3(a_3).float()
is_3(valid_3_tens) # tensor([True, True, True,  ..., True, True, True])
```

Now let's calculate the accuracy:

```Python
accuracy_3s = is_3(valid_3_tens).float() .mean()
accuracy_7s = (1 - is_3(valid_7_tens).float()).mean()
accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2
# (tensor(0.9168), tensor(0.9854), tensor(0.9511)) - over 90%!
```

This is great, but only the start. Especially because this is just the 

## Stochastic Gradient Descent (SGD)

Instead of trying to find the similarity between an image and an "ideal image," we could instead look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8. This can be represented as a function and set of weight values for each possible category—for instance the probability of being the number 8:

```python
def pr_eight(x,w): return (x*w).sum()
```

^ In this, x is a vector (the image pixel values), and w is the weight vector (of the pixel values). Using this method, we can slowly update the way we make our function, by improving after calculating the loss.

To be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:

1. _Initialize_ the weights. - Random Values!
2. For each image, use these weights to _predict_ whether it appears to be a 3 or a 7.
3. Based on these predictions, calculate how good the model is (its _loss_).
4. Calculate the _gradient_, which measures for each weight, how changing that weight would change the loss - The Stochastic Gradient Descent
5. _Step_ (that is, change) all the weights based on that calculation. - Updating the weights
6. Go back to the step 2, and _repeat_ the process.
7. Iterate until you decide to _stop_ the training process (for instance, because the model is good enough or you don't want to wait any longer).


Before applying these steps to our image classification problem, let's illustrate what they look like in a simpler case. First we will define a very simple function, the quadratic—let's pretend that this is our loss function, and `x` is a weight parameter of the function:

![[Screenshot 2024-01-18 at 6.12.46 PM.png]]

Essentially, if we want to find the local minima, we need to get the slope of a point on this curve. Then, by taking small steps, we can find ourselves at the bottom of this.

Jeremy goes on:

The one magic step is the bit where we calculate the gradients. As we mentioned, we use calculus as a performance optimization; it allows us to more quickly calculate whether our loss will go up or down when we adjust our parameters up or down. In other words, the gradients will tell us how much we have to change each weight to make our model better.

Quick Refresher: https://www.khanacademy.org/math/differential-calculus/dc-diff-intro

The big thing you need to conceptualize is that the rate of change of a function is measure by a derivative.

### Calculating Gradients

Amazingly enough, PyTorch is able to automatically compute the derivative of nearly any function! What's more, it does it very fast. Most of the time, it will be at least as fast as any derivative function that you can create by hand. Let's see an example.

Here's some code:

```Python
xt = tensor(3.).requires_grad_()
```

Notice the special method `requires_grad_`? That's the magical incantation we use to tell PyTorch that we want to calculate gradients with respect to that variable at that value. It is essentially tagging the variable, so PyTorch will remember to keep track of how to compute gradients of the other, direct calculations on it that you will ask for.

Something to note - the gradient is typically a function in math, rather than the values. In ML, the gradient usually refers to the values of the functions derivative at a particular arg.

Notice how PyTorch prints not just the value calculated, but also a note that it has a gradient function it'll be using to calculate our gradients when needed:

```Python
yt = f(xt)

yt # tensor(9., grad_fn=<PowBackward0>)
```

Next, tell PyTorch to calculate gradient

```Python
yt.backward()
```

Above, this is backpropagation, which is the name given to the process of calculating the derivative of each layer.

This is called the "backward pass" of the network, as opposed to the "forward pass," which is where the activations are calculated.

Now displaying it

```Python 
xt.grad # tensor(6.)
```

If you remember your high school calculus rules, the derivative of `x**2` is `2*x`, and we have `x=3`, so the gradients should be `2*3=6`, which is what PyTorch calculated for us!

### Stepping With a Learning Rate

Using a learning rate, which is essentially a smaller number you multiply a learning rate by, is a really good way of controlling the "step". Often this is called an optimizer step.

```Python
w -= gradient(w) * lr
```

Notice how we _subtract_ the `gradient * lr` from the parameter to update it. This allows us to adjust the parameter in the direction of the slope by increasing the parameter when the slope is negative and decreasing the parameter when the slope is positive. We want to adjust our parameters in the direction of the slope because our goal in deep learning is to _minimize_ the loss. - This is something I may have to review

Learning rate that is too slow = too many steps
Learning rate that is too high = can make the loss worse or bounce around!

### End to End SGD

To be Continued!